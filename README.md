# A paper per day for 100 days
I read a paper every day for 100 days, documenting them here.

- Day 1 - [ImageNet Classification with Deep Convolutional Neural Networks](https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf)
- Day 2 - [Attention is all you need](https://arxiv.org/abs/1706.03762)
- Day 3 - [Visualizing and Understanding Convolutional Networks](https://arxiv.org/abs/1311.2901)
- Day 4 - [A ConvNet for the 2020s](https://arxiv.org/pdf/2201.03545.pdf)
- Day 5 - [The Matrix Calculus you need for Deep Learning](https://arxiv.org/pdf/1802.01528.pdf)
- Day 6 - [Deepface Closing the Gap to Human-level Performance](https://research.facebook.com/file/266870805034649/deepface-closing-the-gap-to-human-level-performance-in-face-verification.pdf)
- Day 7 - [Improving Language Understanding by Generative Pre-Training (GPT 1)](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)
- Day 8 - [Language Models are Unsupervised Multitask Learners (GPT 2)](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)
- Day 9 - [Language Models are Few-Shot Learners (GPT 3)](https://arxiv.org/abs/2005.14165)
- Day 10 - [Going Deeper with Convolutions](https://arxiv.org/abs/1409.4842)
- Day 11 - [Rethinking the Inception Architecture for Computer Vision](https://arxiv.org/abs/1512.00567)
- Day 12 - [Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning](https://arxiv.org/abs/1602.07261)
- Day 13 - [Long Short Term Memory](https://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.13.634)
- Day 14 - [Deep Residual Learning for Image Recognition - Resnet](https://arxiv.org/abs/1512.03385)
- Day 15 - [U-Net: Convolutional Networks for Biomedical Image Segmentation](https://arxiv.org/abs/1505.04597)
- Day 16 - [Very Deep Convolutional Networks for Large-Scale Image Recognition](https://arxiv.org/abs/1409.1556)
- Day 17 - [A Decision-Theoretic Generalization of on-Line Learning and an Application to Boosting](https://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.32.8918)
- Day 18 - [Bagging Predictors](https://link.springer.com/article/10.1023/A:1018054314350)
# Brief image generating model section
### GANs (first state of the art)
- Day 19 - [Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks](https://arxiv.org/abs/1511.06434)
- Day 20 - [Improved Techniques for Training GANs](https://arxiv.org/abs/1606.03498)
- Day 21 - [Conditional Generative Adversarial Nets](https://arxiv.org/abs/1411.1784)
- Day 22 - [Generative Adversarial Networks (the original paper)](https://arxiv.org/abs/1406.2661)
### Diffusion Models
- Day 19 - [Diffusion Models Beat GANs on Image Synthesis](https://arxiv.org/abs/2105.05233)
- Day 20 - [Denoising Diffusion Probabilistic Models](https://arxiv.org/abs/2006.11239)
- Day 21 - [Understanding Diffusion Models: A Unified Perspective](https://arxiv.org/abs/2208.11970)
- Day 22 - [Deep Unsupervised Learning using Nonequilibrium Thermodynamics (the original paper)](https://arxiv.org/abs/1503.03585)
- Day 23 - [High-Resolution Image Synthesis with Latent Diffusion Models](https://arxiv.org/abs/2112.10752)
- Day 24 - [Hierarchical Text-Conditional Image Generation with CLIP Latents](https://arxiv.org/pdf/2204.06125.pdf)
- Day 25 - [Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding](https://arxiv.org/pdf/2205.11487.pdf)
- Day 26 - [Adding Conditional Control to Text-to-Image Diffusion Models](https://arxiv.org/abs/2302.05543)
### Autoregressive Model
- Day 27 - [Scaling Autoregressive Multi-Modal Models: Pretraining and Instruction Tuning](https://ai.meta.com/research/publications/scaling-autoregressive-multi-modal-models-pretraining-and-instruction-tuning/)
### More famous papers
- Day 28 - [Greedy function approximation: A gradient boosting machine.](https://projecteuclid.org/journals/annals-of-statistics/volume-29/issue-5/Greedy-function-approximation-A-gradient-boosting-machine/10.1214/aos/1013203451.full)
- Day 29 - [XGBoost: A Scalable Tree Boosting System](https://arxiv.org/abs/1603.02754)
- Day 30 - [Random Forests](https://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.125.5395)
- Day 31 - [Mastering the game of Go with Deep Neural Networks & Tree Search](https://www.deepmind.com/publications/mastering-the-game-of-go-with-deep-neural-networks-tree-search)
- Day 32 - [Generally capable agents emerge from open-ended play](https://www.deepmind.com/blog/generally-capable-agents-emerge-from-open-ended-play)
- Day 33 - [Highly accurate protein structure prediction with AlphaFold](https://www.deepmind.com/publications/highly-accurate-protein-structure-prediction-with-alphafold)
- Day 34 - [Adam: A Method for Stochastic Optimization](https://arxiv.org/abs/1412.6980)
- Day 35 - [Autograd: Effortless Gradients in Numpy](https://indico.ijclab.in2p3.fr/event/2914/contributions/6483/subcontributions/180/attachments/6060/7185/automl-short.pdf)
- Day 36 - [Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift](http://proceedings.mlr.press/v37/ioffe15.html)
- Day 37 - [Dropout: A Simple Way to Prevent Neural Networks from Overfitting](https://jmlr.org/papers/v15/srivastava14a.html)
- Day 38 - [Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm](https://arxiv.org/abs/1712.01815)



# Other things to read  
- Reinforcement learning papers
- NLP papers
- Ethics papers
- ViTs (Vision Transformers)
- Statistics (PCA, fisher vectors and fisher kernels, boltzmann machines)
- SIFT+FV networks
- GELUs and other activation functions
- https://ai.googleblog.com/2018/03/using-evolutionary-automl-to-discover.html
- https://www.fast.ai/posts/2018-08-10-fastai-diu-imagenet.html
- Mixup, Cutmix, RandAugment, Random Erasing, and regularization scehemes like Stochastic Depth and Label Smoothing
- Inverted bottleneck
- BatchNorm vs LayerNorm
