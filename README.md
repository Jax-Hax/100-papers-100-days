# A paper per day for 100 days
I read a paper every day for 100 days, documenting them here. A ⭐ means that I need to come back to the paper after I know more. A ❌ means that it isn't necessary for you to read, as it was kind of a meh read.

1. [ImageNet Classification with Deep Convolutional Neural Networks](https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf)
1. ⭐[Attention is all you need](https://arxiv.org/abs/1706.03762)
1. [Visualizing and Understanding Convolutional Networks](https://arxiv.org/abs/1311.2901)
1. [A ConvNet for the 2020s](https://arxiv.org/pdf/2201.03545.pdf)
1. ⭐[The Matrix Calculus you need for Deep Learning](https://arxiv.org/pdf/1802.01528.pdf)
1. [Deepface Closing the Gap to Human-level Performance](https://research.facebook.com/file/266870805034649/deepface-closing-the-gap-to-human-level-performance-in-face-verification.pdf)
1. ⭐[Improving Language Understanding by Generative Pre-Training (GPT 1)](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)
1. ⭐[Language Models are Unsupervised Multitask Learners (GPT 2)](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)
1. [Language Models are Few-Shot Learners (GPT 3)](https://arxiv.org/abs/2005.14165)
2. ❌[Character-Level Language Modeling with Deeper Self-Attention](https://arxiv.org/abs/1808.04444)
3. ⭐[Recurrent Neural Networks (RNNs): A gentle Introduction and Overview](https://arxiv.org/abs/1912.05911)
4. ⭐[RWKV: Reinventing RNNs for the Transformer Era](https://arxiv.org/abs/2305.13048)
6. [Pytorch](https://arxiv.org/pdf/1912.01703.pdf)
5. [DIFFEDIT: DIFFUSION-BASED SEMANTIC IMAGE EDITING WITH MASK GUIDANCE](https://arxiv.org/pdf/2210.11427.pdf)
6. [RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control](https://robotics-transformer2.github.io/assets/rt2.pdf)
7. [RT-1: ROBOTICS TRANSFORMER FOR REAL-WORLD CONTROL AT SCALE](https://robotics-transformer.github.io/assets/rt1.pdf)
1. [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)
1. [Pointer networks](https://arxiv.org/abs/1506.03134)
1. [Layer normalization](https://arxiv.org/abs/1607.06450)
1. [Going Deeper with Convolutions](https://arxiv.org/abs/1409.4842)
1. [Rethinking the Inception Architecture for Computer Vision](https://arxiv.org/abs/1512.00567)
1. [Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning](https://arxiv.org/abs/1602.07261)
1. [Long Short Term Memory](https://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.13.634)
1. [Deep Residual Learning for Image Recognition - Resnet](https://arxiv.org/abs/1512.03385)
1. [U-Net: Convolutional Networks for Biomedical Image Segmentation](https://arxiv.org/abs/1505.04597)
1. [Very Deep Convolutional Networks for Large-Scale Image Recognition](https://arxiv.org/abs/1409.1556)
1. [A Decision-Theoretic Generalization of on-Line Learning and an Application to Boosting](https://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.32.8918)
1. [Bagging Predictors](https://link.springer.com/article/10.1023/A:1018054314350)
1. [Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks](https://arxiv.org/abs/1511.06434)
1. [Improved Techniques for Training GANs](https://arxiv.org/abs/1606.03498)
1. [Conditional Generative Adversarial Nets](https://arxiv.org/abs/1411.1784)
1. [Generative Adversarial Networks (the original paper)](https://arxiv.org/abs/1406.2661)
1. [Diffusion Models Beat GANs on Image Synthesis](https://arxiv.org/abs/2105.05233)
1. [Denoising Diffusion Probabilistic Models](https://arxiv.org/abs/2006.11239)
1. [Understanding Diffusion Models: A Unified Perspective](https://arxiv.org/abs/2208.11970)
1. [Deep Unsupervised Learning using Nonequilibrium Thermodynamics (the original paper)](https://arxiv.org/abs/1503.03585)
1. [High-Resolution Image Synthesis with Latent Diffusion Models](https://arxiv.org/abs/2112.10752)
1. [Hierarchical Text-Conditional Image Generation with CLIP Latents](https://arxiv.org/pdf/2204.06125.pdf)
1. [Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding](https://arxiv.org/pdf/2205.11487.pdf)
1. [Adding Conditional Control to Text-to-Image Diffusion Models](https://arxiv.org/abs/2302.05543)
1. [Scaling Autoregressive Multi-Modal Models: Pretraining and Instruction Tuning](https://ai.meta.com/research/publications/scaling-autoregressive-multi-modal-models-pretraining-and-instruction-tuning/)
1. [Greedy function approximation: A gradient boosting machine.](https://projecteuclid.org/journals/annals-of-statistics/volume-29/issue-5/Greedy-function-approximation-A-gradient-boosting-machine/10.1214/aos/1013203451.full)
1. [XGBoost: A Scalable Tree Boosting System](https://arxiv.org/abs/1603.02754)
1. [Random Forests](https://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.125.5395)
1. [Mastering the game of Go with Deep Neural Networks & Tree Search](https://www.deepmind.com/publications/mastering-the-game-of-go-with-deep-neural-networks-tree-search)
1. [Generally capable agents emerge from open-ended play](https://www.deepmind.com/blog/generally-capable-agents-emerge-from-open-ended-play)
1. [Highly accurate protein structure prediction with AlphaFold](https://www.deepmind.com/publications/highly-accurate-protein-structure-prediction-with-alphafold)
1. [Adam: A Method for Stochastic Optimization](https://arxiv.org/abs/1412.6980)
1. [Autograd: Effortless Gradients in Numpy](https://indico.ijclab.in2p3.fr/event/2914/contributions/6483/subcontributions/180/attachments/6060/7185/automl-short.pdf)
1. [Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift](http://proceedings.mlr.press/v37/ioffe15.html)
1. [Dropout: A Simple Way to Prevent Neural Networks from Overfitting](https://jmlr.org/papers/v15/srivastava14a.html)
1. [Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm](https://arxiv.org/abs/1712.01815)
1. [Torch: A modular machine learning software library](https://publications.idiap.ch/attachments/reports/2002/rr02-46.pdf)
1. [Automatic differentiation in PyTorch](https://openreview.net/pdf?id=BJJsrmfCZ)
1. [TensorFlow: A system for large-scale machine learning](https://research.google/pubs/pub45381/)
1. [Experiments on Learning by Back Propagation](https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.70.878&rep=rep1&type=pdf)
1. [Support Vector Networks](https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.15.9362&rep=rep1&type=pdf) maybe idk this one's a bit old
1. [Latent Dirichlet Allocation](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.80.7000&rep=rep1&type=pdf) maybe idk this one's a bit old
1. [Statistical Modeling: The Two Cultures](https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.156.4933&rep=rep1&type=pdf)
1. [Textbooks are all you need](https://arxiv.org/abs/2306.11644)
2. [A Fast Learning Algorithm for Deep Belief Nets](https://www.cs.utoronto.ca/~hinton/absps/ncfast.pdf)
3. [Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context](https://arxiv.org/abs/1901.02860)
4. [Subject-Diffusion](https://github.com/OPPO-Mente-Lab/Subject-Diffusion)
5. [Generating long sequences with sparse transformers](https://arxiv.org/abs/1904.10509)
6. [Improving Multi-Task Deep Neural Networks via Knowledge Distillation for Natural Language Understanding](https://arxiv.org/abs/1904.09482)
7. [Improved Techniques for Training GANs](https://arxiv.org/abs/1606.03498)
8. [Dynamic Evaluation of Neural Sequence Models](https://arxiv.org/abs/1709.07432)
9. [Grandmaster level in StarCraft II using multi-agent reinforcement learning](https://www.nature.com/articles/s41586-019-1724-z.epdf?author_access_token=lZH3nqPYtWJXfDA10W0CNNRgN0jAjWel9jnR3ZoTv0PSZcPzJFGNAZhOlk4deBCKzKm70KfinloafEF1bCCXL6IIHHgKaDkaTkBcTEv7aT-wqDoG1VeO9-wO3GEoAMF9bAOt7mJ0RWQnRVMbyfgH9A%3D%3D)
10. [Efficient Transformers: A survey](https://arxiv.org/pdf/2009.06732.pdf)
11. [FlashAttention](https://arxiv.org/abs/2205.14135)
12. [FlashAttention 2](https://tridao.me/publications/flash2/flash2.pdf)
13. [SpikeGPT](https://arxiv.org/abs/2302.13939)
14. [Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling](https://arxiv.org/abs/1412.3555)
15. [Learning to Model the World with Language](https://arxiv.org/abs/2308.01399)
16. [Learning Transferable Visual Models From Natural Language Supervision](https://arxiv.org/abs/2103.00020)

# Other things to read  
- Reinforcement learning papers
- NLP papers
- Ethics papers
- ViTs (Vision Transformers)
- Statistics (PCA, fisher vectors and fisher kernels, boltzmann machines)
- SIFT+FV networks
- GELUs and other activation functions
- facial recognition
- 3d model representation
- https://ai.googleblog.com/2018/03/using-evolutionary-automl-to-discover.html
- https://www.fast.ai/posts/2018-08-10-fastai-diu-imagenet.html
- Mixup, Cutmix, RandAugment, Random Erasing, and regularization scehemes like Stochastic Depth and Label Smoothing
- Inverted bottleneck
- BatchNorm vs LayerNorm
- https://www.reddit.com/r/MachineLearning/comments/hj4cx/comment/c1vt6ny/
- Bishop's Pattern Recognition and Machine Learning
- http://alumni.media.mit.edu/~tpminka/statlearn/glossary/
- Statistical Pattern Recognition by Andrew Webb
- Programming Collective Intelligence
- https://www.d2l.ai/
